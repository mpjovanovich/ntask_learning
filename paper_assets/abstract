Temporal difference learning models can perform poorly when optimal policy cannot be determined solely by sensory input. Converging evidence from studies of working memory suggest that humans form abstract mental representations that align with significant features of a task, allowing such conditions to be overcome. We here present n-task learning, an algorithm that utilizes abstract representations to form multiple policies based around a common set of sensory inputs. Inputs to a temporal difference learning algorithm are combined conjunctively with an abstract input that comes to represent attention to a task. The agent completes a dynamic categorization problem that is marked by frequently recurring tasks. The algorithm learns the correct number of tasks as well as when to switch from one task representation to another, even when the input is identical across all tasks. Task performance is shown to be optimal only when an appropriate number of abstract representations is used.
